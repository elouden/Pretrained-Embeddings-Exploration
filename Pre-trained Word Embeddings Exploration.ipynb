{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#GloVe-Vector-Importing\" data-toc-modified-id=\"GloVe-Vector-Importing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>GloVe Vector Importing</a></span></li><li><span><a href=\"#Word-Vector-Dictionary\" data-toc-modified-id=\"Word-Vector-Dictionary-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Word-Vector Dictionary</a></span></li><li><span><a href=\"#Target-Vocabulary---Anna-Karenina\" data-toc-modified-id=\"Target-Vocabulary---Anna-Karenina-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Target Vocabulary - Anna Karenina</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Preprocessing</a></span></li></ul></li><li><span><a href=\"#PyTorch-Embedding-Layer\" data-toc-modified-id=\"PyTorch-Embedding-Layer-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>PyTorch Embedding Layer</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import bcolz\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Vector Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6B.50.dat\n"
     ]
    }
   ],
   "source": [
    "# Data file name\n",
    "datFileName = '6B.50.dat'\n",
    "print(datFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "#vectors = bcolz.carray(np.zeros(1), rootdir=f'{glove_path}/6B.50.dat', mode='w')\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=f'6B.50.dat', mode='w')\n",
    "\n",
    "#with open(f'{glove_path}/glove.6B.50d.txt', 'rb') as f:\n",
    "with open(f'glove.6B.50d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "    \n",
    "#vectors = bcolz.carray(vectors[1:].reshape((400000, 50)), rootdir=f'{glove_path}/6B.50.dat', mode='w')\n",
    "#vectors.flush()\n",
    "#pickle.dump(words, open(f'{glove_path}/6B.50_words.pkl', 'wb'))\n",
    "#pickle.dump(word2idx, open(f'{glove_path}/6B.50_idx.pkl', 'wb'))\n",
    "\n",
    "vectors = bcolz.carray(vectors[1:].reshape((400000, 50)), rootdir=f'6B.50.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'6B.50_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'6B.50_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code works if bcolz is not available"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = []\n",
    "\n",
    "fileName = 'glove.6B.50d.txt'\n",
    "with open(fileName, 'rb') as f:\n",
    "    for l in f:\n",
    "        if idx < 50: # just grab the first 50 encodings for now\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            words.append(word)\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "            vectors.append(vect)\n",
    "    \n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-Vector Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.open(f'{datFileName}')[:]\n",
    "words = pickle.load(open(f'6B.50_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'6B.50_idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Vocabulary - Anna Karenina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    anna_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion in the Oblonskys' house. The wife had\\ndiscovered that the husband was carrying on\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anna_text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict to turn punctuation into a token.\n",
    "punct2token = {'.': '<PERIOD>',\n",
    "                ',': '<COMMA>',\n",
    "                '\"': '<QUOTATION_MARK>',\n",
    "                ';': '<SEMICOLON>',\n",
    "                '!': '<EXCLAMATION_MARK>',\n",
    "                '?': '<QUESTION_MARK>',\n",
    "                '(': '<LEFT_PAREN>',\n",
    "                ')': '<RIGHT_PAREN>',\n",
    "                '--': ' <HYPHENS> ',\n",
    "                '-': '<DASH>',\n",
    "                '?': '<QUESTION_MARK>',\n",
    "                '\\n': '<NEW_LINE>',\n",
    "                ':': ' <COLON> '}\n",
    "\n",
    "# Tokenize the punctuation\n",
    "for punct, token in punct2token.items():\n",
    "    anna_text = anna_text.replace(punct, ' {} '.format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1 <NEW_LINE>  <NEW_LINE>  <NEW_LINE> Happy families are all alike <SEMICOLON>  every unhappy family is unhappy in its own <NEW_LINE> way <PERIOD>  <NEW_LINE>  <NEW_LINE> Everything was in conf'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anna_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and make all ensure all text is lower case\n",
    "anna_text = anna_text.lower()\n",
    "anna_text = anna_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter',\n",
       " '1',\n",
       " '<new_line>',\n",
       " '<new_line>',\n",
       " '<new_line>',\n",
       " 'happy',\n",
       " 'families',\n",
       " 'are',\n",
       " 'all',\n",
       " 'alike',\n",
       " '<semicolon>',\n",
       " 'every',\n",
       " 'unhappy',\n",
       " 'family',\n",
       " 'is',\n",
       " 'unhappy',\n",
       " 'in',\n",
       " 'its',\n",
       " 'own',\n",
       " '<new_line>',\n",
       " 'way',\n",
       " '<period>',\n",
       " '<new_line>',\n",
       " '<new_line>',\n",
       " 'everything',\n",
       " 'was',\n",
       " 'in',\n",
       " 'confusion',\n",
       " 'in',\n",
       " 'the',\n",
       " \"oblonskys'\",\n",
       " 'house',\n",
       " '<period>',\n",
       " 'the',\n",
       " 'wife',\n",
       " 'had',\n",
       " '<new_line>',\n",
       " 'discovered',\n",
       " 'that',\n",
       " 'the',\n",
       " 'husband',\n",
       " 'was',\n",
       " 'carrying',\n",
       " 'on',\n",
       " 'an',\n",
       " 'intrigue',\n",
       " 'with',\n",
       " 'a',\n",
       " 'french',\n",
       " '<new_line>',\n",
       " 'girl',\n",
       " '<comma>',\n",
       " 'who',\n",
       " 'had',\n",
       " 'been',\n",
       " 'a',\n",
       " 'governess',\n",
       " 'in',\n",
       " 'their',\n",
       " 'family',\n",
       " '<comma>',\n",
       " 'and',\n",
       " 'she',\n",
       " 'had',\n",
       " 'announced',\n",
       " 'to',\n",
       " '<new_line>',\n",
       " 'her',\n",
       " 'husband',\n",
       " 'that',\n",
       " 'she',\n",
       " 'could',\n",
       " 'not',\n",
       " 'go',\n",
       " 'on',\n",
       " 'living',\n",
       " 'in',\n",
       " 'the',\n",
       " 'same',\n",
       " 'house',\n",
       " 'with',\n",
       " 'him',\n",
       " '<period>',\n",
       " '<new_line>',\n",
       " 'this',\n",
       " 'position',\n",
       " 'of',\n",
       " 'affairs',\n",
       " 'had',\n",
       " 'now',\n",
       " 'lasted',\n",
       " 'three',\n",
       " 'days',\n",
       " '<comma>',\n",
       " 'and',\n",
       " 'not',\n",
       " 'only',\n",
       " 'the',\n",
       " '<new_line>',\n",
       " 'husband',\n",
       " 'and',\n",
       " 'wife',\n",
       " 'themselves',\n",
       " '<comma>',\n",
       " 'but',\n",
       " 'all',\n",
       " 'the',\n",
       " 'members',\n",
       " 'of',\n",
       " 'their',\n",
       " 'family',\n",
       " 'and',\n",
       " '<new_line>',\n",
       " 'household',\n",
       " '<comma>',\n",
       " 'were',\n",
       " 'painfully',\n",
       " 'conscious',\n",
       " 'of',\n",
       " 'it',\n",
       " '<period>',\n",
       " 'every',\n",
       " 'person',\n",
       " 'in',\n",
       " 'the',\n",
       " 'house',\n",
       " '<new_line>',\n",
       " 'felt',\n",
       " 'that',\n",
       " 'there',\n",
       " 'was',\n",
       " 'no',\n",
       " 'sense',\n",
       " 'in',\n",
       " 'their',\n",
       " 'living',\n",
       " 'together',\n",
       " '<comma>',\n",
       " 'and',\n",
       " 'that',\n",
       " 'the',\n",
       " '<new_line>',\n",
       " 'stray',\n",
       " 'people',\n",
       " 'brought',\n",
       " 'together',\n",
       " 'by',\n",
       " 'chance',\n",
       " 'in',\n",
       " 'any',\n",
       " 'inn',\n",
       " 'had',\n",
       " 'more',\n",
       " 'in',\n",
       " 'common',\n",
       " '<new_line>',\n",
       " 'with',\n",
       " 'one',\n",
       " 'another',\n",
       " 'than',\n",
       " 'they',\n",
       " '<comma>',\n",
       " 'the',\n",
       " 'members',\n",
       " 'of',\n",
       " 'the',\n",
       " 'family',\n",
       " 'and',\n",
       " 'household',\n",
       " 'of',\n",
       " '<new_line>',\n",
       " 'the',\n",
       " 'oblonskys',\n",
       " '<period>',\n",
       " 'the',\n",
       " 'wife',\n",
       " 'did',\n",
       " 'not',\n",
       " 'leave',\n",
       " 'her',\n",
       " 'own',\n",
       " 'room',\n",
       " '<comma>',\n",
       " 'the',\n",
       " 'husband',\n",
       " 'had',\n",
       " 'not',\n",
       " '<new_line>',\n",
       " 'been',\n",
       " 'at',\n",
       " 'home',\n",
       " 'for',\n",
       " 'three',\n",
       " 'days',\n",
       " '<period>',\n",
       " 'the',\n",
       " 'children',\n",
       " 'ran',\n",
       " 'wild',\n",
       " 'all']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anna_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab dictionaries\n",
    "\n",
    "# get the word counts\n",
    "word_counts = Counter(anna_text)\n",
    "    \n",
    "# sort from most to least frequent\n",
    "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "# define the dictionaries\n",
    "idx2word = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "word2idx = {word: ii for ii, word in idx2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply dictionaries to text\n",
    "target_vocab = [word2idx[word] for word in anna_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a pickle file\n",
    "pickle.dump((target_vocab, idx2word, word2idx, punct2token), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply this to our desired dataset's vocabulary (which may be different from that in GloVe).  If the word is in GloVe's vocabulary, load in the pre-trained word vector.  Otherwise, initialize a random vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
